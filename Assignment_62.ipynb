{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-hTOJeegQ05f"
      },
      "outputs": [],
      "source": [
        "#assignment 62"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans 1: Polynomial functions and kernel functions are both used in machine learning algorithms, but they serve different purposes.\n",
        "\n",
        "Polynomial functions are a class of functions that can be used to fit data by constructing a polynomial equation of a given degree. In machine learning, polynomial functions are often used in regression problems to model the relationship between the input variables and the output variable. Polynomial regression is a specific type of regression analysis where the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial.\n",
        "\n",
        "Kernel functions, on the other hand, are used in various machine learning algorithms, such as support vector machines (SVMs) and kernel principal component analysis (KPCA). In SVMs, a kernel function is used to transform the input data into a higher-dimensional feature space, where it can be more easily separated into different classes. The kernel function calculates the similarity between pairs of data points in this higher-dimensional space. Examples of kernel functions include linear kernels, polynomial kernels, and radial basis function (RBF) kernels.\n",
        "\n",
        "While polynomial functions can be used as kernel functions, not all kernel functions are polynomial functions. For example, the RBF kernel is a popular kernel function in SVMs, but it is not a polynomial function. In general, kernel functions provide a more flexible way to represent non-linear relationships between variables in machine learning algorithms, while polynomial functions are useful for fitting data with a polynomial equation of a given degree."
      ],
      "metadata": {
        "id": "DYYlEssXRBgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans 2:\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # we only take the first two features\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "svm_poly_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_poly\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "])\n",
        "svm_poly_pipeline.fit(X_train, y_train)\n",
        "y_pred = svm_poly_pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSFoVEcxQ_St",
        "outputId": "9df93f4c-df84-4e85-de58-9b6d62fb8e23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 75.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans 3:Support Vector Regression (SVR), epsilon is a hyperparameter that controls the width of the margin around the predicted function. Increasing the value of epsilon will increase the width of the margin, allowing more points to be within the margin and still considered as correctly predicted by the model.\n",
        "\n",
        "As the width of the margin increases, the number of support vectors in the model may also increase. Support vectors are the data points that lie on the margin or within the margin and are important for determining the position and shape of the decision boundary.\n",
        "\n",
        "When epsilon is small, the margin is narrow, and the model may only have a few support vectors. As epsilon increases, the margin widens, and more points may become support vectors.\n",
        "\n",
        "However, the relationship between epsilon and the number of support vectors is not necessarily linear or straightforward. The number of support vectors also depends on other factors such as the complexity of the problem and the kernel function used.\n",
        "\n",
        "In general, increasing the value of epsilon in SVR can lead to an increase in the number of support vectors in the model, but the exact relationship between the two depends on various factors and may require experimentation and tuning to find the optimal value for epsilon."
      ],
      "metadata": {
        "id": "cndNS0EhQ_rv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c044GC0KRnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans 4: The performance of Support Vector Regression (SVR) is highly dependent on the choice of kernel function, C parameter, epsilon parameter, and gamma parameter. Let's discuss each parameter and its effect on SVR performance:\n",
        "\n",
        "Kernel Function: Kernel functions are used to transform the input data into a higher-dimensional feature space, where it can be more easily separated into different classes. The choice of kernel function affects the shape of the decision boundary and can impact the accuracy of the SVR model. Examples of kernel functions include linear, polynomial, and radial basis function (RBF) kernels.\n",
        "Linear Kernel: A linear kernel can be useful when the data is linearly separable, and the goal is to obtain a linear regression model.\n",
        "Polynomial Kernel: A polynomial kernel can be useful when the data has a non-linear relationship between the input and output variables, and the degree of non-linearity is not too high.\n",
        "RBF Kernel: An RBF kernel is useful when the data has a high degree of non-linearity or when it is difficult to define a specific kernel function. The choice of gamma parameter can affect the smoothness of the decision boundary.\n",
        "C Parameter: The C parameter is a hyperparameter that controls the trade-off between achieving a low training error and a low testing error. A large value of C will lead to a model with a smaller margin but more training errors, while a small value of C will lead to a larger margin but more testing errors.\n",
        "Large C value: A large value of C can be useful when the training set has a low error rate, and overfitting is a concern.\n",
        "Small C value: A small value of C can be useful when the training set has a high error rate, and generalization is a concern.\n",
        "Epsilon Parameter: The epsilon parameter is a hyperparameter that controls the width of the margin around the predicted function. A large value of epsilon will lead to a wider margin and a more tolerant model, while a small value of epsilon will lead to a narrower margin and a less tolerant model.\n",
        "Large Epsilon value: A large value of epsilon can be useful when the model needs to be more tolerant to errors in the training data or when the data has a lot of noise.\n",
        "Small Epsilon value: A small value of epsilon can be useful when the model needs to be less tolerant to errors in the training data or when the data has low noise.\n",
        "Gamma Parameter: The gamma parameter is a hyperparameter that controls the smoothness of the decision boundary. A large value of gamma will lead to a more complex decision boundary and potentially overfitting, while a small value of gamma will lead to a smoother decision boundary and potentially underfitting.\n",
        "Large Gamma value: A large value of gamma can be useful when the data has a high degree of non-linearity or when the model needs to be more complex to fit the data well.\n",
        "Small Gamma value: A small value of gamma can be useful when the data has a low degree of non-linearity or when the model needs to be simpler to generalize well."
      ],
      "metadata": {
        "id": "4exWFYB1Rs0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib\n",
        "\n",
        "# load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# preprocess the data using standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# create an instance of the SVC classifier and train it on the training data\n",
        "svc = SVC()\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# use the trained classifier to predict the labels of the testing data\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the classifier using accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# tune the hyperparameters of the SVC classifier using GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10], 'kernel': ['rbf']}\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n",
        "\n",
        "# train the tuned classifier on the entire dataset\n",
        "tuned_svc = SVC(C=10, gamma=0.1, kernel='rbf')\n",
        "tuned_svc.fit(X, y)\n",
        "\n",
        "# save the trained classifier to a file for future use\n",
        "joblib.dump(tuned_svc, 'svc_classifier.joblib')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRn0LjODRuIA",
        "outputId": "19c0290f-fbe7-482f-e82e-bead958e3d27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  1.0\n",
            "Best parameters:  {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Best score:  0.95\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svc_classifier.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OxMUpDXdR1Q5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}