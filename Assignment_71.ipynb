{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iDCLsECjLRzB"
      },
      "outputs": [],
      "source": [
        "#assignment 71"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans 1:Gradient Boosting Regression is a machine learning technique used for regression problems. It is an ensemble method that combines multiple weak learning models, typically decision trees, to create a stronger prediction model.\n",
        "\n",
        "In Gradient Boosting Regression, the algorithm starts by building an initial model and then iteratively adds more models, with each new model being trained to correct the errors of the previous model. The models are trained sequentially, with each subsequent model attempting to minimize the residual error of the previous model.\n",
        "\n",
        "The process of building the ensemble of models is done by computing the gradient of the loss function with respect to the predictions of the previous model. This gradient is then used to update the parameters of the next model, such that it can better predict the residuals of the previous model.\n",
        "\n",
        "The final prediction of the ensemble model is the sum of the predictions of all the individual models. Gradient Boosting Regression has been shown to be effective in a wide range of applications, and is particularly useful when dealing with non-linear relationships between the input features and the target variable."
      ],
      "metadata": {
        "id": "pZGs0GGrLYwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans 2:\n",
        "import numpy as np\n",
        "\n",
        "# Define input features and target variable\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y, depth=0)\n",
        "        \n",
        "    def _build_tree(self, X, y, depth):\n",
        "        # Stop building tree if max depth is reached or all samples have the same target value\n",
        "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
        "            return np.mean(y)\n",
        "        \n",
        "        # Find the best feature to split on based on lowest mean squared error\n",
        "        mse = float('inf')\n",
        "        for feature in range(X.shape[1]):\n",
        "            for threshold in np.unique(X[:, feature]):\n",
        "                left_y = y[X[:, feature] < threshold]\n",
        "                right_y = y[X[:, feature] >= threshold]\n",
        "                \n",
        "                if len(left_y) == 0 or len(right_y) == 0:\n",
        "                    continue\n",
        "                \n",
        "                left_mse = np.mean(np.square(left_y - np.mean(left_y)))\n",
        "                right_mse = np.mean(np.square(right_y - np.mean(right_y)))\n",
        "                total_mse = left_mse + right_mse\n",
        "                \n",
        "                if total_mse < mse:\n",
        "                    mse = total_mse\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "        \n",
        "        # Split the dataset based on the best feature and threshold\n",
        "        left_X = X[X[:, best_feature] < best_threshold]\n",
        "        left_y = y[X[:, best_feature] < best_threshold]\n",
        "        right_X = X[X[:, best_feature] >= best_threshold]\n",
        "        right_y = y[X[:, best_feature] >= best_threshold]\n",
        "        \n",
        "        # Recursively build left and right subtrees\n",
        "        left_tree = self._build_tree(left_X, left_y, depth+1)\n",
        "        right_tree = self._build_tree(right_X, right_y, depth+1)\n",
        "        \n",
        "        # Combine the subtrees into a single tree\n",
        "        return {'feature': best_feature, 'threshold': best_threshold, 'left': left_tree, 'right': right_tree}\n",
        "        \n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_tree(x, self.tree) for x in X])\n",
        "        \n",
        "    def _predict_tree(self, x, tree):\n",
        "        if isinstance(tree, float):\n",
        "            return tree\n",
        "        \n",
        "        if x[tree['feature']] < tree['threshold']:\n",
        "            return self._predict_tree(x, tree['left'])\n",
        "        else:\n",
        "            return self._predict_tree(x, tree['right'])\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max\n"
      ],
      "metadata": {
        "id": "D00aQK_9LXtW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ans 3: \n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from itertools import product\n",
        "\n",
        "def grid_search(X, y, estimator, param_grid, cv=5):\n",
        "    best_params = None\n",
        "    best_score = float('-inf')\n",
        "    \n",
        "    for params in product(*param_grid.values()):\n",
        "        params = dict(zip(param_grid.keys(), params))\n",
        "        model = estimator(**params)\n",
        "        \n",
        "        scores = []\n",
        "        for train_idx, val_idx in KFold(n_splits=cv).split(X):\n",
        "            X_train, y_train = X[train_idx], y[train_idx]\n",
        "            X_val, y_val = X[val_idx], y[val_idx]\n",
        "            \n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_val)\n",
        "            score = r2_score(y_val, y_pred)\n",
        "            scores.append(score)\n",
        "            \n",
        "        avg_score = np.mean(scores)\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            best_params = params\n",
        "    \n",
        "    return best_params, best_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Define input features and target variable\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
        "\n",
        "# Define hyperparameters to search over\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'learning_rate': [0.1, 0.01],\n",
        "    'max_depth': [2, 3, 4]\n",
        "}\n",
        "\n",
        "# Perform grid search over hyperparameters\n",
        "best_params, best_score = grid_search(X, y, GradientBoostingRegressor, param_grid)\n",
        "\n",
        "# Train final model with best hyperparameters\n",
        "model = GradientBoostingRegressor(**best_params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Evaluate final model performance\n",
        "y_pred = model.predict(X)\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "print(\"Best R-squared score:\", best_score)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "id": "1pS6K43TLtIM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans 4:In Gradient Boosting, a weak learner is a model that performs only slightly better than random guessing on a given dataset. Weak learners are typically simple models with low complexity, such as decision trees with a single split or linear regression models.\n",
        "\n",
        "In the context of Gradient Boosting, weak learners are combined together to form a strong ensemble model. Each weak learner is trained on the residuals (i.e., the difference between the predicted and true values) of the previous model in the ensemble. By iteratively adding weak learners and optimizing the ensemble's predictions, the Gradient Boosting algorithm is able to create a strong predictive model that can generalize well to new data.\n",
        "\n",
        "The key advantage of using weak learners in Gradient Boosting is that it allows the model to be flexible and adaptive to complex and non-linear relationships in the data, while avoiding overfitting. By focusing on improving the residuals, rather than fitting the training data perfectly, Gradient Boosting can prevent the model from memorizing the training data and improve its ability to generalize to new data."
      ],
      "metadata": {
        "id": "tlje9d2cMFH5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xowzHty8L5Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans 5: The intuition behind the Gradient Boosting algorithm is to iteratively improve the predictions of a weak learner by adding new models to the ensemble, with each model trained to correct the errors of the previous models.\n",
        "\n",
        "At a high level, Gradient Boosting works as follows:\n",
        "\n",
        "First, a simple model (i.e., a weak learner) is trained on the training data.\n",
        "The model's predictions are compared to the true values, and the differences between the two are calculated.\n",
        "A new model is then trained to predict the residuals (i.e., the differences between the predicted and true values) of the previous model.\n",
        "The predictions of the new model are added to the predictions of the previous model, creating a new and improved set of predictions.\n",
        "Steps 2-4 are repeated iteratively, with each new model trained on the residuals of the previous model, until the algorithm reaches a predetermined stopping criterion (e.g., a maximum number of iterations or a minimum improvement in performance).\n",
        "The key intuition behind Gradient Boosting is that by iteratively correcting the errors of the previous models, the algorithm is able to create a strong and accurate predictive model. The weak learners used in Gradient Boosting are typically simple models with low complexity, which allows the algorithm to avoid overfitting and generalize well to new data."
      ],
      "metadata": {
        "id": "8QgL4BjeMPFU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S_Xrc05MMQb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans 6: The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new models to the ensemble, with each model trained to correct the errors of the previous models. The general steps for building an ensemble in Gradient Boosting are as follows:\n",
        "\n",
        "Initialize the ensemble: The algorithm starts by initializing the ensemble with a simple model, typically a decision tree with a single split or a linear regression model.\n",
        "\n",
        "Compute the residuals: The predictions of the current ensemble are compared to the true values, and the differences between the two (i.e., the residuals) are computed.\n",
        "\n",
        "Train a new model: A new weak learner is trained to predict the residuals of the previous model. The goal is to find a new model that can accurately predict the errors of the current ensemble, and thus improve the overall predictions of the ensemble.\n",
        "\n",
        "Add the new model to the ensemble: The predictions of the new model are added to the predictions of the current ensemble, creating a new set of predictions that better fit the true values.\n",
        "\n",
        "Iterate: Steps 2-4 are repeated iteratively, with each new model trained on the residuals of the previous model, until a stopping criterion is met (e.g., a maximum number of iterations or a minimum improvement in performance)."
      ],
      "metadata": {
        "id": "G1VtgClEMUaW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_GhqYpKoMVdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans 7: The mathematical intuition behind the Gradient Boosting algorithm can be constructed using the following steps:\n",
        "\n",
        "Define the objective function: In Gradient Boosting, the objective function is typically a loss function that measures the difference between the predicted and true values of the target variable. For regression problems, the mean squared error (MSE) or mean absolute error (MAE) are commonly used as loss functions.\n",
        "\n",
        "Initialize the ensemble: The algorithm starts by initializing the ensemble with a simple model, typically a constant value that is the average or median of the target variable.\n",
        "\n",
        "Compute the negative gradient of the objective function: The negative gradient of the objective function with respect to the current predictions of the ensemble is computed. This negative gradient represents the direction of steepest descent for the objective function and indicates how much the predictions need to be adjusted to minimize the loss.\n",
        "\n",
        "Train a new model: A new weak learner is trained to predict the negative gradient computed in step 3. This new model is typically a decision tree with a small number of splits, and is trained to approximate the negative gradient as closely as possible.\n",
        "\n",
        "Compute the optimal step size: The optimal step size (also known as the learning rate) is computed by minimizing the objective function with respect to the new model. This step size determines how much the predictions of the new model are added to the predictions of the previous models in the ensemble.\n",
        "\n",
        "Add the new model to the ensemble: The predictions of the new model, scaled by the optimal step size, are added to the predictions of the previous models in the ensemble. This creates a new set of predictions that better fit the true values of the target variable.\n",
        "\n",
        "Iterate: Steps 3-6 are repeated iteratively, with each new model trained on the negative gradient of the previous predictions, until a stopping criterion is met (e.g., a maximum number of iterations or a minimum improvement in performance)."
      ],
      "metadata": {
        "id": "y2vPTmOWMZUI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XX_RJutgMafe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}